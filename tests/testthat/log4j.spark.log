17/04/02 17:19:53 INFO SparkContext: Running Spark version 2.0.0
17/04/02 17:19:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/04/02 17:19:54 INFO SecurityManager: Changing view acls to: akhil
17/04/02 17:19:54 INFO SecurityManager: Changing modify acls to: akhil
17/04/02 17:19:54 INFO SecurityManager: Changing view acls groups to: 
17/04/02 17:19:54 INFO SecurityManager: Changing modify acls groups to: 
17/04/02 17:19:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(akhil); groups with view permissions: Set(); users  with modify permissions: Set(akhil); groups with modify permissions: Set()
17/04/02 17:19:54 INFO Utils: Successfully started service 'sparkDriver' on port 50540.
17/04/02 17:19:54 INFO SparkEnv: Registering MapOutputTracker
17/04/02 17:19:54 INFO SparkEnv: Registering BlockManagerMaster
17/04/02 17:19:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-79b00b17-a426-4c99-a7ec-c789eb083ab4
17/04/02 17:19:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/04/02 17:19:54 INFO SparkEnv: Registering OutputCommitCoordinator
17/04/02 17:19:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/04/02 17:19:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
17/04/02 17:19:54 INFO Utils: Successfully started service 'SparkUI' on port 4042.
17/04/02 17:19:54 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4042
17/04/02 17:19:54 INFO SparkContext: Added JAR file:/home/akhil/R/x86_64-pc-linux-gnu-library/3.3/sparklyr/java/sparklyr-2.0-2.11.jar at spark://127.0.0.1:50540/jars/sparklyr-2.0-2.11.jar with timestamp 1491149994660
17/04/02 17:19:54 INFO Executor: Starting executor ID driver on host localhost
17/04/02 17:19:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40415.
17/04/02 17:19:54 INFO NettyBlockTransferService: Server created on 127.0.0.1:40415
17/04/02 17:19:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 40415)
17/04/02 17:19:54 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:40415 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 40415)
17/04/02 17:19:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 40415)
17/04/02 17:19:54 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/04/02 17:19:55 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/04/02 17:19:55 INFO HiveSharedState: Warehouse path is 'file:/home/akhil/example/crassy/tests/testthat/spark-warehouse'.
17/04/02 17:19:55 INFO NettyUtil: Found Netty's native epoll transport in the classpath, using it
17/04/02 17:19:56 INFO Cluster: New Cassandra host localhost/127.0.0.1:9042 added
17/04/02 17:19:56 INFO CassandraConnector: Connected to Cassandra cluster: Test Cluster
17/04/02 17:19:57 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/04/02 17:19:58 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/04/02 17:19:58 INFO ObjectStore: ObjectStore, initialize called
17/04/02 17:19:58 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/04/02 17:19:58 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/04/02 17:19:59 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/04/02 17:20:00 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:20:00 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:20:01 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:20:01 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:20:01 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/04/02 17:20:01 INFO ObjectStore: Initialized ObjectStore
17/04/02 17:20:01 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/04/02 17:20:01 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/04/02 17:20:01 INFO HiveMetaStore: Added admin role in metastore
17/04/02 17:20:01 INFO HiveMetaStore: Added public role in metastore
17/04/02 17:20:01 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/04/02 17:20:02 INFO HiveMetaStore: 0: get_all_databases
17/04/02 17:20:02 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_all_databases	
17/04/02 17:20:02 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/04/02 17:20:02 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/04/02 17:20:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:20:02 INFO SessionState: Created local directory: /tmp/cb76b753-78d4-4d32-8a2c-2a4408cf8854_resources
17/04/02 17:20:02 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/cb76b753-78d4-4d32-8a2c-2a4408cf8854
17/04/02 17:20:02 INFO SessionState: Created local directory: /tmp/akhil/cb76b753-78d4-4d32-8a2c-2a4408cf8854
17/04/02 17:20:02 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/cb76b753-78d4-4d32-8a2c-2a4408cf8854/_tmp_space.db
17/04/02 17:20:02 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/home/akhil/example/crassy/tests/testthat/spark-warehouse
17/04/02 17:20:02 INFO SessionState: Created local directory: /tmp/d602cab2-c940-43c7-9b57-af0d1e03e632_resources
17/04/02 17:20:02 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/d602cab2-c940-43c7-9b57-af0d1e03e632
17/04/02 17:20:02 INFO SessionState: Created local directory: /tmp/akhil/d602cab2-c940-43c7-9b57-af0d1e03e632
17/04/02 17:20:02 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/d602cab2-c940-43c7-9b57-af0d1e03e632/_tmp_space.db
17/04/02 17:20:02 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/home/akhil/example/crassy/tests/testthat/spark-warehouse
17/04/02 17:20:02 INFO HiveMetaStore: 0: create_database: Database(name:default, description:default database, locationUri:file:/home/akhil/example/crassy/tests/testthat/spark-warehouse, parameters:{})
17/04/02 17:20:02 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/home/akhil/example/crassy/tests/testthat/spark-warehouse, parameters:{})	
17/04/02 17:20:03 INFO SparkSqlParser: Parsing command: spk_tbl
17/04/02 17:20:03 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/04/02 17:20:03 INFO HiveMetaStore: 0: get_database: default
17/04/02 17:20:03 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_database: default	
17/04/02 17:20:03 INFO HiveMetaStore: 0: get_database: default
17/04/02 17:20:03 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_database: default	
17/04/02 17:20:03 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/04/02 17:20:03 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/04/02 17:20:03 INFO CassandraConnector: Disconnected from Cassandra cluster: Test Cluster
17/04/02 17:20:03 INFO CodeGenerator: Code generated in 280.689243 ms
17/04/02 17:20:03 INFO SparkContext: Starting job: collect at utils.scala:59
17/04/02 17:20:03 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
17/04/02 17:20:03 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
17/04/02 17:20:03 INFO DAGScheduler: Parents of final stage: List()
17/04/02 17:20:03 INFO DAGScheduler: Missing parents: List()
17/04/02 17:20:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at map at utils.scala:56), which has no missing parents
17/04/02 17:20:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 8.3 KB, free 366.3 MB)
17/04/02 17:20:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.4 KB, free 366.3 MB)
17/04/02 17:20:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:40415 (size: 4.4 KB, free: 366.3 MB)
17/04/02 17:20:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
17/04/02 17:20:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at map at utils.scala:56)
17/04/02 17:20:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/04/02 17:20:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5685 bytes)
17/04/02 17:20:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/04/02 17:20:04 INFO Executor: Fetching spark://127.0.0.1:50540/jars/sparklyr-2.0-2.11.jar with timestamp 1491149994660
17/04/02 17:20:04 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:50540 after 17 ms (0 ms spent in bootstraps)
17/04/02 17:20:04 INFO Utils: Fetching spark://127.0.0.1:50540/jars/sparklyr-2.0-2.11.jar to /tmp/spark-e9ad8aa4-9a9c-46b2-afe9-3a24a354c51d/userFiles-28586826-83a0-4261-9036-ed35e18c9050/fetchFileTemp4290370343339440197.tmp
17/04/02 17:20:04 INFO Executor: Adding file:/tmp/spark-e9ad8aa4-9a9c-46b2-afe9-3a24a354c51d/userFiles-28586826-83a0-4261-9036-ed35e18c9050/sparklyr-2.0-2.11.jar to class loader
17/04/02 17:20:04 INFO CodeGenerator: Code generated in 14.422141 ms
17/04/02 17:20:04 INFO CodeGenerator: Code generated in 10.508621 ms
17/04/02 17:20:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1072 bytes result sent to driver
17/04/02 17:20:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 191 ms on localhost (1/1)
17/04/02 17:20:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/04/02 17:20:04 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) finished in 0.203 s
17/04/02 17:20:04 INFO DAGScheduler: Job 0 finished: collect at utils.scala:59, took 0.317327 s
17/04/02 17:20:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 70.7 KB, free 366.2 MB)
17/04/02 17:20:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.9 KB, free 366.2 MB)
17/04/02 17:20:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:40415 (size: 22.9 KB, free: 366.3 MB)
17/04/02 17:20:04 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:-2
17/04/02 17:20:04 INFO FileInputFormat: Total input paths to process : 1
17/04/02 17:20:04 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:-2
17/04/02 17:20:04 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/04/02 17:20:04 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:-2)
17/04/02 17:20:04 INFO DAGScheduler: Parents of final stage: List()
17/04/02 17:20:04 INFO DAGScheduler: Missing parents: List()
17/04/02 17:20:04 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/04/02 17:20:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 366.2 MB)
17/04/02 17:20:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.0 KB, free 366.2 MB)
17/04/02 17:20:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:40415 (size: 2.0 KB, free: 366.3 MB)
17/04/02 17:20:04 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1012
17/04/02 17:20:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:-2)
17/04/02 17:20:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/04/02 17:20:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5502 bytes)
17/04/02 17:20:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/04/02 17:20:04 INFO HadoopRDD: Input split: file:/tmp/RtmpUpfPp4/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2013
17/04/02 17:20:04 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/04/02 17:20:04 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/04/02 17:20:04 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/04/02 17:20:04 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/04/02 17:20:04 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/04/02 17:20:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 986 bytes result sent to driver
17/04/02 17:20:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 42 ms on localhost (1/1)
17/04/02 17:20:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/04/02 17:20:04 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:-2) finished in 0.042 s
17/04/02 17:20:04 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:-2, took 0.052696 s
17/04/02 17:20:04 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:40415 in memory (size: 2.0 KB, free: 366.3 MB)
17/04/02 17:20:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 232.8 KB, free 366.0 MB)
17/04/02 17:20:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:40415 in memory (size: 4.4 KB, free: 366.3 MB)
17/04/02 17:20:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.9 KB, free 366.0 MB)
17/04/02 17:20:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:40415 (size: 22.9 KB, free: 366.3 MB)
17/04/02 17:20:04 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:-2
17/04/02 17:20:04 INFO SparkSqlParser: Parsing command: iris
17/04/02 17:20:04 INFO SparkSqlParser: Parsing command: CACHE TABLE `iris`
17/04/02 17:20:04 INFO SparkSqlParser: Parsing command: `iris`
17/04/02 17:20:04 INFO FileSourceStrategy: Pruning directories with: 
17/04/02 17:20:04 INFO FileSourceStrategy: Post-Scan Filters: 
17/04/02 17:20:04 INFO FileSourceStrategy: Pruned Data Schema: struct<Sepal_Length: double, Sepal_Width: double, Petal_Length: double, Petal_Width: double, Species: string ... 3 more fields>
17/04/02 17:20:04 INFO FileSourceStrategy: Pushed Filters: 
17/04/02 17:20:04 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 269.0 KB, free 365.7 MB)
17/04/02 17:20:04 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 23.4 KB, free 365.7 MB)
17/04/02 17:20:04 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:40415 (size: 23.4 KB, free: 366.2 MB)
17/04/02 17:20:04 INFO SparkContext: Created broadcast 4 from sql at NativeMethodAccessorImpl.java:-2
17/04/02 17:20:05 INFO FileSourceStrategy: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/04/02 17:20:05 INFO CodeGenerator: Code generated in 6.554592 ms
17/04/02 17:20:05 INFO CodeGenerator: Code generated in 9.857752 ms
17/04/02 17:20:05 INFO CodeGenerator: Code generated in 15.982262 ms
17/04/02 17:20:05 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:-2
17/04/02 17:20:05 INFO DAGScheduler: Registering RDD 20 (sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:20:05 INFO DAGScheduler: Got job 2 (sql at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/04/02 17:20:05 INFO DAGScheduler: Final stage: ResultStage 3 (sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:20:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
17/04/02 17:20:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
17/04/02 17:20:05 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[20] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/04/02 17:20:05 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 16.4 KB, free 365.7 MB)
17/04/02 17:20:05 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KB, free 365.6 MB)
17/04/02 17:20:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:40415 (size: 7.7 KB, free: 366.2 MB)
17/04/02 17:20:05 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1012
17/04/02 17:20:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[20] at sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:20:05 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/04/02 17:20:05 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5989 bytes)
17/04/02 17:20:05 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/04/02 17:20:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 127.0.0.1:40415 in memory (size: 22.9 KB, free: 366.2 MB)
17/04/02 17:20:05 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:40415 in memory (size: 22.9 KB, free: 366.3 MB)
17/04/02 17:20:05 INFO ContextCleaner: Cleaned accumulator 93
17/04/02 17:20:05 INFO ContextCleaner: Cleaned accumulator 1
17/04/02 17:20:05 INFO ContextCleaner: Cleaned accumulator 0
17/04/02 17:20:05 INFO FileScanRDD: Reading File path: file:///tmp/RtmpUpfPp4/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv, range: 0-4026, partition values: [empty row]
17/04/02 17:20:05 INFO CodeGenerator: Code generated in 12.92139 ms
17/04/02 17:20:05 INFO MemoryStore: Block rdd_17_0 stored as values in memory (estimated size 5.6 KB, free 366.0 MB)
17/04/02 17:20:05 INFO BlockManagerInfo: Added rdd_17_0 in memory on 127.0.0.1:40415 (size: 5.6 KB, free: 366.3 MB)
17/04/02 17:20:05 INFO CodeGenerator: Code generated in 3.253917 ms
17/04/02 17:20:05 INFO CodeGenerator: Code generated in 18.869735 ms
17/04/02 17:20:05 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3567 bytes result sent to driver
17/04/02 17:20:05 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 320 ms on localhost (1/1)
17/04/02 17:20:05 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/04/02 17:20:05 INFO DAGScheduler: ShuffleMapStage 2 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.322 s
17/04/02 17:20:05 INFO DAGScheduler: looking for newly runnable stages
17/04/02 17:20:05 INFO DAGScheduler: running: Set()
17/04/02 17:20:05 INFO DAGScheduler: waiting: Set(ResultStage 3)
17/04/02 17:20:05 INFO DAGScheduler: failed: Set()
17/04/02 17:20:05 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[23] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/04/02 17:20:05 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.0 KB, free 366.0 MB)
17/04/02 17:20:05 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.7 KB, free 366.0 MB)
17/04/02 17:20:05 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:40415 (size: 3.7 KB, free: 366.3 MB)
17/04/02 17:20:05 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1012
17/04/02 17:20:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[23] at sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:20:05 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/04/02 17:20:05 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0, ANY, 5343 bytes)
17/04/02 17:20:05 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
17/04/02 17:20:05 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/04/02 17:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
17/04/02 17:20:05 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1873 bytes result sent to driver
17/04/02 17:20:05 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 26 ms on localhost (1/1)
17/04/02 17:20:05 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/04/02 17:20:05 INFO DAGScheduler: ResultStage 3 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.027 s
17/04/02 17:20:05 INFO DAGScheduler: Job 2 finished: sql at NativeMethodAccessorImpl.java:-2, took 0.391462 s
17/04/02 17:20:05 INFO CodeGenerator: Code generated in 7.383947 ms
17/04/02 17:20:05 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `iris`
17/04/02 17:20:05 INFO SparkSqlParser: Parsing command: count(1)
17/04/02 17:20:05 INFO CodeGenerator: Code generated in 32.189853 ms
17/04/02 17:20:05 INFO CodeGenerator: Code generated in 11.123663 ms
17/04/02 17:20:05 INFO SparkContext: Starting job: collect at utils.scala:195
17/04/02 17:20:05 INFO DAGScheduler: Registering RDD 27 (collect at utils.scala:195)
17/04/02 17:20:05 INFO DAGScheduler: Got job 3 (collect at utils.scala:195) with 1 output partitions
17/04/02 17:20:05 INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:195)
17/04/02 17:20:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
17/04/02 17:20:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)
17/04/02 17:20:05 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[27] at collect at utils.scala:195), which has no missing parents
17/04/02 17:20:05 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 15.6 KB, free 366.0 MB)
17/04/02 17:20:05 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.4 KB, free 366.0 MB)
17/04/02 17:20:05 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:40415 (size: 7.4 KB, free: 366.3 MB)
17/04/02 17:20:05 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1012
17/04/02 17:20:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[27] at collect at utils.scala:195)
17/04/02 17:20:05 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/04/02 17:20:05 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5981 bytes)
17/04/02 17:20:05 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
17/04/02 17:20:05 INFO BlockManager: Found block rdd_17_0 locally
17/04/02 17:20:05 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2141 bytes result sent to driver
17/04/02 17:20:05 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 11 ms on localhost (1/1)
17/04/02 17:20:05 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/04/02 17:20:05 INFO DAGScheduler: ShuffleMapStage 4 (collect at utils.scala:195) finished in 0.013 s
17/04/02 17:20:05 INFO DAGScheduler: looking for newly runnable stages
17/04/02 17:20:05 INFO DAGScheduler: running: Set()
17/04/02 17:20:05 INFO DAGScheduler: waiting: Set(ResultStage 5)
17/04/02 17:20:05 INFO DAGScheduler: failed: Set()
17/04/02 17:20:05 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[30] at collect at utils.scala:195), which has no missing parents
17/04/02 17:20:05 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 9.5 KB, free 365.9 MB)
17/04/02 17:20:05 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.1 KB, free 365.9 MB)
17/04/02 17:20:05 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:40415 (size: 4.1 KB, free: 366.2 MB)
17/04/02 17:20:05 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1012
17/04/02 17:20:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[30] at collect at utils.scala:195)
17/04/02 17:20:05 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
17/04/02 17:20:05 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0, ANY, 5335 bytes)
17/04/02 17:20:05 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
17/04/02 17:20:05 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/04/02 17:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/04/02 17:20:05 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2189 bytes result sent to driver
17/04/02 17:20:05 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 7 ms on localhost (1/1)
17/04/02 17:20:05 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/04/02 17:20:05 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:195) finished in 0.008 s
17/04/02 17:20:05 INFO DAGScheduler: Job 3 finished: collect at utils.scala:195, took 0.052275 s
17/04/02 17:20:05 INFO SparkSqlParser: Parsing command: SELECT *
FROM `iris` AS `zzz1`
WHERE (0 = 1)
17/04/02 17:20:06 INFO SparkSqlParser: Parsing command: Sepal_Length
17/04/02 17:20:06 INFO SparkSqlParser: Parsing command: Sepal_Width
17/04/02 17:20:06 INFO SparkSqlParser: Parsing command: Petal_Length
17/04/02 17:20:06 INFO SparkSqlParser: Parsing command: Petal_Width
17/04/02 17:20:06 INFO SparkSqlParser: Parsing command: Species
17/04/02 17:20:06 INFO SparkSqlParser: Parsing command: SELECT *
FROM `iris`
17/04/02 17:20:06 INFO Cluster: New Cassandra host localhost/127.0.0.1:9042 added
17/04/02 17:20:06 INFO CassandraConnector: Connected to Cassandra cluster: Test Cluster
17/04/02 17:20:06 INFO SparkContext: Invoking stop() from shutdown hook
17/04/02 17:20:06 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4042
17/04/02 17:20:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/04/02 17:20:06 INFO MemoryStore: MemoryStore cleared
17/04/02 17:20:06 INFO BlockManager: BlockManager stopped
17/04/02 17:20:06 INFO BlockManagerMaster: BlockManagerMaster stopped
17/04/02 17:20:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/04/02 17:20:06 INFO SparkContext: Successfully stopped SparkContext
17/04/02 17:20:06 INFO ShutdownHookManager: Shutdown hook called
17/04/02 17:20:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-e9ad8aa4-9a9c-46b2-afe9-3a24a354c51d
17/04/02 17:20:08 INFO CassandraConnector: Disconnected from Cassandra cluster: Test Cluster
17/04/02 17:20:08 INFO SerialShutdownHooks: Successfully executed shutdown hook: Clearing session cache for C* connector
17/04/02 17:20:56 INFO SparkContext: Running Spark version 2.0.0
17/04/02 17:20:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/04/02 17:20:56 INFO SecurityManager: Changing view acls to: akhil
17/04/02 17:20:56 INFO SecurityManager: Changing modify acls to: akhil
17/04/02 17:20:56 INFO SecurityManager: Changing view acls groups to: 
17/04/02 17:20:56 INFO SecurityManager: Changing modify acls groups to: 
17/04/02 17:20:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(akhil); groups with view permissions: Set(); users  with modify permissions: Set(akhil); groups with modify permissions: Set()
17/04/02 17:20:56 INFO Utils: Successfully started service 'sparkDriver' on port 52426.
17/04/02 17:20:56 INFO SparkEnv: Registering MapOutputTracker
17/04/02 17:20:56 INFO SparkEnv: Registering BlockManagerMaster
17/04/02 17:20:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-03ee0050-2341-4443-9e9a-d519485bcb4a
17/04/02 17:20:56 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/04/02 17:20:56 INFO SparkEnv: Registering OutputCommitCoordinator
17/04/02 17:20:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/04/02 17:20:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
17/04/02 17:20:56 INFO Utils: Successfully started service 'SparkUI' on port 4042.
17/04/02 17:20:56 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4042
17/04/02 17:20:56 INFO SparkContext: Added JAR file:/home/akhil/R/x86_64-pc-linux-gnu-library/3.3/sparklyr/java/sparklyr-2.0-2.11.jar at spark://127.0.0.1:52426/jars/sparklyr-2.0-2.11.jar with timestamp 1491150056978
17/04/02 17:20:57 INFO Executor: Starting executor ID driver on host localhost
17/04/02 17:20:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54815.
17/04/02 17:20:57 INFO NettyBlockTransferService: Server created on 127.0.0.1:54815
17/04/02 17:20:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 54815)
17/04/02 17:20:57 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:54815 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 54815)
17/04/02 17:20:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 54815)
17/04/02 17:20:57 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/04/02 17:20:57 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/04/02 17:20:57 INFO HiveSharedState: Warehouse path is 'file:/home/akhil/example/crassy/tests/testthat/spark-warehouse'.
17/04/02 17:20:57 INFO NettyUtil: Found Netty's native epoll transport in the classpath, using it
17/04/02 17:20:58 INFO Cluster: New Cassandra host localhost/127.0.0.1:9042 added
17/04/02 17:20:58 INFO CassandraConnector: Connected to Cassandra cluster: Test Cluster
17/04/02 17:20:59 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/04/02 17:21:00 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/04/02 17:21:00 INFO ObjectStore: ObjectStore, initialize called
17/04/02 17:21:00 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/04/02 17:21:00 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/04/02 17:21:01 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/04/02 17:21:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:21:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:21:03 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:21:03 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:21:03 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/04/02 17:21:03 INFO ObjectStore: Initialized ObjectStore
17/04/02 17:21:03 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/04/02 17:21:03 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/04/02 17:21:03 INFO HiveMetaStore: Added admin role in metastore
17/04/02 17:21:03 INFO HiveMetaStore: Added public role in metastore
17/04/02 17:21:03 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/04/02 17:21:04 INFO HiveMetaStore: 0: get_all_databases
17/04/02 17:21:04 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_all_databases	
17/04/02 17:21:04 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/04/02 17:21:04 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/04/02 17:21:04 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:21:04 INFO SessionState: Created local directory: /tmp/52419d82-945e-4fd7-b7d5-81d11ef741a2_resources
17/04/02 17:21:04 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/52419d82-945e-4fd7-b7d5-81d11ef741a2
17/04/02 17:21:04 INFO SessionState: Created local directory: /tmp/akhil/52419d82-945e-4fd7-b7d5-81d11ef741a2
17/04/02 17:21:04 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/52419d82-945e-4fd7-b7d5-81d11ef741a2/_tmp_space.db
17/04/02 17:21:04 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/home/akhil/example/crassy/tests/testthat/spark-warehouse
17/04/02 17:21:04 INFO SessionState: Created local directory: /tmp/c1f51505-1725-47d3-989e-f18eb680e3cc_resources
17/04/02 17:21:04 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/c1f51505-1725-47d3-989e-f18eb680e3cc
17/04/02 17:21:04 INFO SessionState: Created local directory: /tmp/akhil/c1f51505-1725-47d3-989e-f18eb680e3cc
17/04/02 17:21:04 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/c1f51505-1725-47d3-989e-f18eb680e3cc/_tmp_space.db
17/04/02 17:21:04 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/home/akhil/example/crassy/tests/testthat/spark-warehouse
17/04/02 17:21:04 INFO HiveMetaStore: 0: create_database: Database(name:default, description:default database, locationUri:file:/home/akhil/example/crassy/tests/testthat/spark-warehouse, parameters:{})
17/04/02 17:21:04 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/home/akhil/example/crassy/tests/testthat/spark-warehouse, parameters:{})	
17/04/02 17:21:04 INFO SparkSqlParser: Parsing command: spk_tbl
17/04/02 17:21:05 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/04/02 17:21:05 INFO HiveMetaStore: 0: get_database: default
17/04/02 17:21:05 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_database: default	
17/04/02 17:21:05 INFO HiveMetaStore: 0: get_database: default
17/04/02 17:21:05 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_database: default	
17/04/02 17:21:05 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/04/02 17:21:05 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/04/02 17:21:05 INFO CodeGenerator: Code generated in 245.896628 ms
17/04/02 17:21:05 INFO CassandraConnector: Disconnected from Cassandra cluster: Test Cluster
17/04/02 17:21:05 INFO SparkContext: Starting job: collect at utils.scala:59
17/04/02 17:21:05 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
17/04/02 17:21:05 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
17/04/02 17:21:05 INFO DAGScheduler: Parents of final stage: List()
17/04/02 17:21:05 INFO DAGScheduler: Missing parents: List()
17/04/02 17:21:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at map at utils.scala:56), which has no missing parents
17/04/02 17:21:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 8.3 KB, free 366.3 MB)
17/04/02 17:21:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.4 KB, free 366.3 MB)
17/04/02 17:21:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:54815 (size: 4.4 KB, free: 366.3 MB)
17/04/02 17:21:06 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
17/04/02 17:21:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at map at utils.scala:56)
17/04/02 17:21:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/04/02 17:21:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5685 bytes)
17/04/02 17:21:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/04/02 17:21:06 INFO Executor: Fetching spark://127.0.0.1:52426/jars/sparklyr-2.0-2.11.jar with timestamp 1491150056978
17/04/02 17:21:06 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:52426 after 6 ms (0 ms spent in bootstraps)
17/04/02 17:21:06 INFO Utils: Fetching spark://127.0.0.1:52426/jars/sparklyr-2.0-2.11.jar to /tmp/spark-3071de10-a4c5-4e1b-96f7-2691319e26d4/userFiles-00667dc8-45d2-439c-aae5-5c5725cc83b3/fetchFileTemp4251575069229947431.tmp
17/04/02 17:21:06 INFO Executor: Adding file:/tmp/spark-3071de10-a4c5-4e1b-96f7-2691319e26d4/userFiles-00667dc8-45d2-439c-aae5-5c5725cc83b3/sparklyr-2.0-2.11.jar to class loader
17/04/02 17:21:06 INFO CodeGenerator: Code generated in 14.458844 ms
17/04/02 17:21:06 INFO CodeGenerator: Code generated in 11.3483 ms
17/04/02 17:21:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1159 bytes result sent to driver
17/04/02 17:21:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 198 ms on localhost (1/1)
17/04/02 17:21:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/04/02 17:21:06 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) finished in 0.213 s
17/04/02 17:21:06 INFO DAGScheduler: Job 0 finished: collect at utils.scala:59, took 0.342423 s
17/04/02 17:21:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 70.7 KB, free 366.2 MB)
17/04/02 17:21:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.9 KB, free 366.2 MB)
17/04/02 17:21:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:54815 (size: 22.9 KB, free: 366.3 MB)
17/04/02 17:21:06 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:-2
17/04/02 17:21:06 INFO FileInputFormat: Total input paths to process : 1
17/04/02 17:21:06 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:-2
17/04/02 17:21:06 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/04/02 17:21:06 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:-2)
17/04/02 17:21:06 INFO DAGScheduler: Parents of final stage: List()
17/04/02 17:21:06 INFO DAGScheduler: Missing parents: List()
17/04/02 17:21:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/04/02 17:21:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 366.2 MB)
17/04/02 17:21:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.0 KB, free 366.2 MB)
17/04/02 17:21:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:54815 (size: 2.0 KB, free: 366.3 MB)
17/04/02 17:21:06 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1012
17/04/02 17:21:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:-2)
17/04/02 17:21:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/04/02 17:21:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5502 bytes)
17/04/02 17:21:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/04/02 17:21:06 INFO HadoopRDD: Input split: file:/tmp/RtmpUpFBwr/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2013
17/04/02 17:21:06 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/04/02 17:21:06 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/04/02 17:21:06 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/04/02 17:21:06 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/04/02 17:21:06 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/04/02 17:21:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 986 bytes result sent to driver
17/04/02 17:21:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 48 ms on localhost (1/1)
17/04/02 17:21:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/04/02 17:21:06 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:-2) finished in 0.040 s
17/04/02 17:21:06 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:-2, took 0.058057 s
17/04/02 17:21:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 232.8 KB, free 366.0 MB)
17/04/02 17:21:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.9 KB, free 365.9 MB)
17/04/02 17:21:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:54815 (size: 22.9 KB, free: 366.2 MB)
17/04/02 17:21:06 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:-2
17/04/02 17:21:06 INFO SparkSqlParser: Parsing command: iris
17/04/02 17:21:06 INFO SparkSqlParser: Parsing command: CACHE TABLE `iris`
17/04/02 17:21:06 INFO SparkSqlParser: Parsing command: `iris`
17/04/02 17:21:06 INFO FileSourceStrategy: Pruning directories with: 
17/04/02 17:21:06 INFO FileSourceStrategy: Post-Scan Filters: 
17/04/02 17:21:06 INFO FileSourceStrategy: Pruned Data Schema: struct<Sepal_Length: double, Sepal_Width: double, Petal_Length: double, Petal_Width: double, Species: string ... 3 more fields>
17/04/02 17:21:06 INFO FileSourceStrategy: Pushed Filters: 
17/04/02 17:21:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 269.0 KB, free 365.7 MB)
17/04/02 17:21:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 23.4 KB, free 365.7 MB)
17/04/02 17:21:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:54815 (size: 23.4 KB, free: 366.2 MB)
17/04/02 17:21:06 INFO SparkContext: Created broadcast 4 from sql at NativeMethodAccessorImpl.java:-2
17/04/02 17:21:06 INFO FileSourceStrategy: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/04/02 17:21:06 INFO CodeGenerator: Code generated in 6.148148 ms
17/04/02 17:21:07 INFO ContextCleaner: Cleaned accumulator 0
17/04/02 17:21:07 INFO ContextCleaner: Cleaned accumulator 1
17/04/02 17:21:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:54815 in memory (size: 4.4 KB, free: 366.2 MB)
17/04/02 17:21:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 127.0.0.1:54815 in memory (size: 22.9 KB, free: 366.3 MB)
17/04/02 17:21:07 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:54815 in memory (size: 2.0 KB, free: 366.3 MB)
17/04/02 17:21:07 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:54815 in memory (size: 22.9 KB, free: 366.3 MB)
17/04/02 17:21:07 INFO CodeGenerator: Code generated in 11.175071 ms
17/04/02 17:21:07 INFO CodeGenerator: Code generated in 7.595972 ms
17/04/02 17:21:07 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:-2
17/04/02 17:21:07 INFO DAGScheduler: Registering RDD 20 (sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:21:07 INFO DAGScheduler: Got job 2 (sql at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/04/02 17:21:07 INFO DAGScheduler: Final stage: ResultStage 3 (sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:21:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
17/04/02 17:21:07 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
17/04/02 17:21:07 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[20] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/04/02 17:21:07 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 16.4 KB, free 366.0 MB)
17/04/02 17:21:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KB, free 366.0 MB)
17/04/02 17:21:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:54815 (size: 7.7 KB, free: 366.3 MB)
17/04/02 17:21:07 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1012
17/04/02 17:21:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[20] at sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:21:07 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/04/02 17:21:07 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5989 bytes)
17/04/02 17:21:07 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/04/02 17:21:07 INFO ContextCleaner: Cleaned accumulator 93
17/04/02 17:21:07 INFO FileScanRDD: Reading File path: file:///tmp/RtmpUpFBwr/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv, range: 0-4026, partition values: [empty row]
17/04/02 17:21:07 INFO CodeGenerator: Code generated in 40.688535 ms
17/04/02 17:21:07 INFO MemoryStore: Block rdd_17_0 stored as values in memory (estimated size 5.6 KB, free 366.0 MB)
17/04/02 17:21:07 INFO BlockManagerInfo: Added rdd_17_0 in memory on 127.0.0.1:54815 (size: 5.6 KB, free: 366.3 MB)
17/04/02 17:21:07 INFO CodeGenerator: Code generated in 4.507081 ms
17/04/02 17:21:07 INFO CodeGenerator: Code generated in 18.769686 ms
17/04/02 17:21:07 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3553 bytes result sent to driver
17/04/02 17:21:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 402 ms on localhost (1/1)
17/04/02 17:21:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/04/02 17:21:07 INFO DAGScheduler: ShuffleMapStage 2 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.405 s
17/04/02 17:21:07 INFO DAGScheduler: looking for newly runnable stages
17/04/02 17:21:07 INFO DAGScheduler: running: Set()
17/04/02 17:21:07 INFO DAGScheduler: waiting: Set(ResultStage 3)
17/04/02 17:21:07 INFO DAGScheduler: failed: Set()
17/04/02 17:21:07 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[23] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/04/02 17:21:07 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.0 KB, free 366.0 MB)
17/04/02 17:21:07 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.7 KB, free 366.0 MB)
17/04/02 17:21:07 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:54815 (size: 3.7 KB, free: 366.3 MB)
17/04/02 17:21:07 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1012
17/04/02 17:21:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[23] at sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:21:07 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/04/02 17:21:07 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0, ANY, 5343 bytes)
17/04/02 17:21:07 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
17/04/02 17:21:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/04/02 17:21:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
17/04/02 17:21:07 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1873 bytes result sent to driver
17/04/02 17:21:07 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 27 ms on localhost (1/1)
17/04/02 17:21:07 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/04/02 17:21:07 INFO DAGScheduler: ResultStage 3 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.028 s
17/04/02 17:21:07 INFO DAGScheduler: Job 2 finished: sql at NativeMethodAccessorImpl.java:-2, took 0.503367 s
17/04/02 17:21:07 INFO CodeGenerator: Code generated in 11.443625 ms
17/04/02 17:21:07 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `iris`
17/04/02 17:21:07 INFO SparkSqlParser: Parsing command: count(1)
17/04/02 17:21:07 INFO CodeGenerator: Code generated in 21.249451 ms
17/04/02 17:21:07 INFO CodeGenerator: Code generated in 8.196392 ms
17/04/02 17:21:07 INFO SparkContext: Starting job: collect at utils.scala:195
17/04/02 17:21:07 INFO DAGScheduler: Registering RDD 27 (collect at utils.scala:195)
17/04/02 17:21:07 INFO DAGScheduler: Got job 3 (collect at utils.scala:195) with 1 output partitions
17/04/02 17:21:07 INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:195)
17/04/02 17:21:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
17/04/02 17:21:07 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)
17/04/02 17:21:07 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[27] at collect at utils.scala:195), which has no missing parents
17/04/02 17:21:07 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 15.6 KB, free 366.0 MB)
17/04/02 17:21:07 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.4 KB, free 366.0 MB)
17/04/02 17:21:07 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:54815 (size: 7.4 KB, free: 366.3 MB)
17/04/02 17:21:07 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1012
17/04/02 17:21:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[27] at collect at utils.scala:195)
17/04/02 17:21:07 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/04/02 17:21:07 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5981 bytes)
17/04/02 17:21:07 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
17/04/02 17:21:07 INFO BlockManager: Found block rdd_17_0 locally
17/04/02 17:21:07 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2141 bytes result sent to driver
17/04/02 17:21:07 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 15 ms on localhost (1/1)
17/04/02 17:21:07 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/04/02 17:21:07 INFO DAGScheduler: ShuffleMapStage 4 (collect at utils.scala:195) finished in 0.016 s
17/04/02 17:21:07 INFO DAGScheduler: looking for newly runnable stages
17/04/02 17:21:07 INFO DAGScheduler: running: Set()
17/04/02 17:21:07 INFO DAGScheduler: waiting: Set(ResultStage 5)
17/04/02 17:21:07 INFO DAGScheduler: failed: Set()
17/04/02 17:21:07 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[30] at collect at utils.scala:195), which has no missing parents
17/04/02 17:21:07 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 9.5 KB, free 365.9 MB)
17/04/02 17:21:07 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.1 KB, free 365.9 MB)
17/04/02 17:21:07 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:54815 (size: 4.1 KB, free: 366.2 MB)
17/04/02 17:21:07 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1012
17/04/02 17:21:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[30] at collect at utils.scala:195)
17/04/02 17:21:07 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
17/04/02 17:21:07 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0, ANY, 5335 bytes)
17/04/02 17:21:07 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
17/04/02 17:21:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/04/02 17:21:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/04/02 17:21:07 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2189 bytes result sent to driver
17/04/02 17:21:07 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 15 ms on localhost (1/1)
17/04/02 17:21:07 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/04/02 17:21:07 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:195) finished in 0.016 s
17/04/02 17:21:07 INFO DAGScheduler: Job 3 finished: collect at utils.scala:195, took 0.053172 s
17/04/02 17:21:08 INFO SparkSqlParser: Parsing command: SELECT *
FROM `iris` AS `zzz1`
WHERE (0 = 1)
17/04/02 17:21:08 INFO SparkSqlParser: Parsing command: Sepal_Length
17/04/02 17:21:08 INFO SparkSqlParser: Parsing command: Sepal_Width
17/04/02 17:21:08 INFO SparkSqlParser: Parsing command: Petal_Length
17/04/02 17:21:08 INFO SparkSqlParser: Parsing command: Petal_Width
17/04/02 17:21:08 INFO SparkSqlParser: Parsing command: Species
17/04/02 17:21:08 INFO SparkSqlParser: Parsing command: SELECT *
FROM `iris`
17/04/02 17:21:08 INFO Cluster: New Cassandra host localhost/127.0.0.1:9042 added
17/04/02 17:21:08 INFO CassandraConnector: Connected to Cassandra cluster: Test Cluster
17/04/02 17:21:08 INFO SparkContext: Invoking stop() from shutdown hook
17/04/02 17:21:08 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4042
17/04/02 17:21:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/04/02 17:21:08 INFO MemoryStore: MemoryStore cleared
17/04/02 17:21:08 INFO BlockManager: BlockManager stopped
17/04/02 17:21:08 INFO BlockManagerMaster: BlockManagerMaster stopped
17/04/02 17:21:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/04/02 17:21:08 INFO SparkContext: Successfully stopped SparkContext
17/04/02 17:21:08 INFO ShutdownHookManager: Shutdown hook called
17/04/02 17:21:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-3071de10-a4c5-4e1b-96f7-2691319e26d4
17/04/02 17:21:10 INFO CassandraConnector: Disconnected from Cassandra cluster: Test Cluster
17/04/02 17:21:10 INFO SerialShutdownHooks: Successfully executed shutdown hook: Clearing session cache for C* connector
17/04/02 17:24:23 INFO SparkContext: Running Spark version 2.0.0
17/04/02 17:24:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/04/02 17:24:23 INFO SecurityManager: Changing view acls to: akhil
17/04/02 17:24:23 INFO SecurityManager: Changing modify acls to: akhil
17/04/02 17:24:23 INFO SecurityManager: Changing view acls groups to: 
17/04/02 17:24:23 INFO SecurityManager: Changing modify acls groups to: 
17/04/02 17:24:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(akhil); groups with view permissions: Set(); users  with modify permissions: Set(akhil); groups with modify permissions: Set()
17/04/02 17:24:24 INFO Utils: Successfully started service 'sparkDriver' on port 37463.
17/04/02 17:24:24 INFO SparkEnv: Registering MapOutputTracker
17/04/02 17:24:24 INFO SparkEnv: Registering BlockManagerMaster
17/04/02 17:24:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bdc43388-8c33-4b02-96c4-47a2e02ba9e9
17/04/02 17:24:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/04/02 17:24:24 INFO SparkEnv: Registering OutputCommitCoordinator
17/04/02 17:24:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/04/02 17:24:24 INFO Utils: Successfully started service 'SparkUI' on port 4041.
17/04/02 17:24:24 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4041
17/04/02 17:24:24 INFO SparkContext: Added JAR file:/home/akhil/R/x86_64-pc-linux-gnu-library/3.3/sparklyr/java/sparklyr-2.0-2.11.jar at spark://127.0.0.1:37463/jars/sparklyr-2.0-2.11.jar with timestamp 1491150264438
17/04/02 17:24:24 INFO Executor: Starting executor ID driver on host localhost
17/04/02 17:24:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40882.
17/04/02 17:24:24 INFO NettyBlockTransferService: Server created on 127.0.0.1:40882
17/04/02 17:24:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 40882)
17/04/02 17:24:24 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:40882 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 40882)
17/04/02 17:24:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 40882)
17/04/02 17:24:24 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/04/02 17:24:24 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/04/02 17:24:25 INFO HiveSharedState: Warehouse path is 'file:/home/akhil/example/crassy/tests/testthat/spark-warehouse'.
17/04/02 17:24:25 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/04/02 17:24:26 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/04/02 17:24:27 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/04/02 17:24:27 INFO ObjectStore: ObjectStore, initialize called
17/04/02 17:24:27 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/04/02 17:24:27 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/04/02 17:24:29 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/04/02 17:24:30 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:24:30 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:24:30 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:24:30 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:24:31 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/04/02 17:24:31 INFO ObjectStore: Initialized ObjectStore
17/04/02 17:24:31 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/04/02 17:24:31 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/04/02 17:24:31 INFO HiveMetaStore: Added admin role in metastore
17/04/02 17:24:31 INFO HiveMetaStore: Added public role in metastore
17/04/02 17:24:32 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/04/02 17:24:32 INFO HiveMetaStore: 0: get_all_databases
17/04/02 17:24:32 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_all_databases	
17/04/02 17:24:32 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/04/02 17:24:32 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/04/02 17:24:32 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:24:32 INFO SessionState: Created local directory: /tmp/ff225a19-66f3-4670-be57-19ee6c2428ec_resources
17/04/02 17:24:32 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/ff225a19-66f3-4670-be57-19ee6c2428ec
17/04/02 17:24:32 INFO SessionState: Created local directory: /tmp/akhil/ff225a19-66f3-4670-be57-19ee6c2428ec
17/04/02 17:24:32 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/ff225a19-66f3-4670-be57-19ee6c2428ec/_tmp_space.db
17/04/02 17:24:32 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/home/akhil/example/crassy/tests/testthat/spark-warehouse
17/04/02 17:24:32 INFO SessionState: Created local directory: /tmp/cb7d5154-2944-41bf-995d-1b8a3458f5e9_resources
17/04/02 17:24:32 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/cb7d5154-2944-41bf-995d-1b8a3458f5e9
17/04/02 17:24:32 INFO SessionState: Created local directory: /tmp/akhil/cb7d5154-2944-41bf-995d-1b8a3458f5e9
17/04/02 17:24:32 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/cb7d5154-2944-41bf-995d-1b8a3458f5e9/_tmp_space.db
17/04/02 17:24:32 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/home/akhil/example/crassy/tests/testthat/spark-warehouse
17/04/02 17:24:33 INFO HiveMetaStore: 0: create_database: Database(name:default, description:default database, locationUri:file:/home/akhil/example/crassy/tests/testthat/spark-warehouse, parameters:{})
17/04/02 17:24:33 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/home/akhil/example/crassy/tests/testthat/spark-warehouse, parameters:{})	
17/04/02 17:24:33 INFO HiveMetaStore: 0: get_database: default
17/04/02 17:24:33 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_database: default	
17/04/02 17:24:33 INFO HiveMetaStore: 0: get_database: default
17/04/02 17:24:33 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_database: default	
17/04/02 17:24:33 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/04/02 17:24:33 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/04/02 17:24:34 INFO CodeGenerator: Code generated in 270.556768 ms
17/04/02 17:24:34 INFO SparkContext: Starting job: collect at utils.scala:59
17/04/02 17:24:34 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
17/04/02 17:24:34 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
17/04/02 17:24:34 INFO DAGScheduler: Parents of final stage: List()
17/04/02 17:24:34 INFO DAGScheduler: Missing parents: List()
17/04/02 17:24:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at map at utils.scala:56), which has no missing parents
17/04/02 17:24:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 8.3 KB, free 366.3 MB)
17/04/02 17:24:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.4 KB, free 366.3 MB)
17/04/02 17:24:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:40882 (size: 4.4 KB, free: 366.3 MB)
17/04/02 17:24:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
17/04/02 17:24:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at map at utils.scala:56)
17/04/02 17:24:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/04/02 17:24:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5414 bytes)
17/04/02 17:24:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/04/02 17:24:34 INFO Executor: Fetching spark://127.0.0.1:37463/jars/sparklyr-2.0-2.11.jar with timestamp 1491150264438
17/04/02 17:24:34 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:37463 after 15 ms (0 ms spent in bootstraps)
17/04/02 17:24:34 INFO Utils: Fetching spark://127.0.0.1:37463/jars/sparklyr-2.0-2.11.jar to /tmp/spark-a63251a5-24a8-4aee-b4b8-d442369f34f9/userFiles-7fe2dc2c-bb1e-496c-ba9c-bb2a0563d233/fetchFileTemp840157694531930039.tmp
17/04/02 17:24:35 INFO Executor: Adding file:/tmp/spark-a63251a5-24a8-4aee-b4b8-d442369f34f9/userFiles-7fe2dc2c-bb1e-496c-ba9c-bb2a0563d233/sparklyr-2.0-2.11.jar to class loader
17/04/02 17:24:35 INFO CodeGenerator: Code generated in 25.286405 ms
17/04/02 17:24:35 INFO CodeGenerator: Code generated in 14.544568 ms
17/04/02 17:24:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1135 bytes result sent to driver
17/04/02 17:24:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 315 ms on localhost (1/1)
17/04/02 17:24:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/04/02 17:24:35 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) finished in 0.332 s
17/04/02 17:24:35 INFO DAGScheduler: Job 0 finished: collect at utils.scala:59, took 0.487477 s
17/04/02 17:24:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 70.7 KB, free 366.2 MB)
17/04/02 17:24:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.9 KB, free 366.2 MB)
17/04/02 17:24:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:40882 (size: 22.9 KB, free: 366.3 MB)
17/04/02 17:24:35 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:-2
17/04/02 17:24:35 INFO FileInputFormat: Total input paths to process : 1
17/04/02 17:24:35 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:-2
17/04/02 17:24:35 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/04/02 17:24:35 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:-2)
17/04/02 17:24:35 INFO DAGScheduler: Parents of final stage: List()
17/04/02 17:24:35 INFO DAGScheduler: Missing parents: List()
17/04/02 17:24:35 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at csv at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/04/02 17:24:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 366.2 MB)
17/04/02 17:24:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.0 KB, free 366.2 MB)
17/04/02 17:24:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:40882 (size: 2.0 KB, free: 366.3 MB)
17/04/02 17:24:35 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1012
17/04/02 17:24:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at csv at NativeMethodAccessorImpl.java:-2)
17/04/02 17:24:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/04/02 17:24:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5502 bytes)
17/04/02 17:24:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/04/02 17:24:35 INFO HadoopRDD: Input split: file:/tmp/RtmpgbJO6d/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2013
17/04/02 17:24:35 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/04/02 17:24:35 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/04/02 17:24:35 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/04/02 17:24:35 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/04/02 17:24:35 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/04/02 17:24:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 986 bytes result sent to driver
17/04/02 17:24:35 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:-2) finished in 0.047 s
17/04/02 17:24:35 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:-2, took 0.057798 s
17/04/02 17:24:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 46 ms on localhost (1/1)
17/04/02 17:24:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/04/02 17:24:35 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 232.8 KB, free 366.0 MB)
17/04/02 17:24:35 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.9 KB, free 365.9 MB)
17/04/02 17:24:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:40882 (size: 22.9 KB, free: 366.2 MB)
17/04/02 17:24:35 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:-2
17/04/02 17:24:35 INFO SparkSqlParser: Parsing command: iris
17/04/02 17:24:35 INFO SparkSqlParser: Parsing command: CACHE TABLE `iris`
17/04/02 17:24:35 INFO SparkSqlParser: Parsing command: `iris`
17/04/02 17:24:35 INFO FileSourceStrategy: Pruning directories with: 
17/04/02 17:24:35 INFO FileSourceStrategy: Post-Scan Filters: 
17/04/02 17:24:35 INFO FileSourceStrategy: Pruned Data Schema: struct<Sepal_Length: double, Sepal_Width: double, Petal_Length: double, Petal_Width: double, Species: string ... 3 more fields>
17/04/02 17:24:35 INFO FileSourceStrategy: Pushed Filters: 
17/04/02 17:24:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 269.0 KB, free 365.7 MB)
17/04/02 17:24:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 23.4 KB, free 365.7 MB)
17/04/02 17:24:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:40882 (size: 23.4 KB, free: 366.2 MB)
17/04/02 17:24:35 INFO SparkContext: Created broadcast 4 from sql at NativeMethodAccessorImpl.java:-2
17/04/02 17:24:35 INFO FileSourceStrategy: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/04/02 17:24:35 INFO CodeGenerator: Code generated in 6.397713 ms
17/04/02 17:24:36 INFO CodeGenerator: Code generated in 13.655292 ms
17/04/02 17:24:36 INFO CodeGenerator: Code generated in 12.075712 ms
17/04/02 17:24:36 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:-2
17/04/02 17:24:36 INFO DAGScheduler: Registering RDD 18 (sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:24:36 INFO DAGScheduler: Got job 2 (sql at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/04/02 17:24:36 INFO DAGScheduler: Final stage: ResultStage 3 (sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:24:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
17/04/02 17:24:36 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
17/04/02 17:24:36 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[18] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/04/02 17:24:36 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 16.4 KB, free 365.6 MB)
17/04/02 17:24:36 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KB, free 365.6 MB)
17/04/02 17:24:36 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:40882 (size: 7.7 KB, free: 366.2 MB)
17/04/02 17:24:36 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1012
17/04/02 17:24:36 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[18] at sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:24:36 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/04/02 17:24:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 127.0.0.1:40882 in memory (size: 22.9 KB, free: 366.2 MB)
17/04/02 17:24:36 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5989 bytes)
17/04/02 17:24:36 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/04/02 17:24:36 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:40882 in memory (size: 2.0 KB, free: 366.2 MB)
17/04/02 17:24:36 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:40882 in memory (size: 22.9 KB, free: 366.3 MB)
17/04/02 17:24:36 INFO ContextCleaner: Cleaned accumulator 93
17/04/02 17:24:36 INFO FileScanRDD: Reading File path: file:///tmp/RtmpgbJO6d/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv, range: 0-4026, partition values: [empty row]
17/04/02 17:24:36 INFO CodeGenerator: Code generated in 10.58976 ms
17/04/02 17:24:36 INFO MemoryStore: Block rdd_15_0 stored as values in memory (estimated size 5.6 KB, free 366.0 MB)
17/04/02 17:24:36 INFO BlockManagerInfo: Added rdd_15_0 in memory on 127.0.0.1:40882 (size: 5.6 KB, free: 366.3 MB)
17/04/02 17:24:36 INFO CodeGenerator: Code generated in 4.544836 ms
17/04/02 17:24:36 INFO CodeGenerator: Code generated in 22.010535 ms
17/04/02 17:24:36 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3567 bytes result sent to driver
17/04/02 17:24:36 INFO DAGScheduler: ShuffleMapStage 2 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.242 s
17/04/02 17:24:36 INFO DAGScheduler: looking for newly runnable stages
17/04/02 17:24:36 INFO DAGScheduler: running: Set()
17/04/02 17:24:36 INFO DAGScheduler: waiting: Set(ResultStage 3)
17/04/02 17:24:36 INFO DAGScheduler: failed: Set()
17/04/02 17:24:36 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 241 ms on localhost (1/1)
17/04/02 17:24:36 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/04/02 17:24:36 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[21] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/04/02 17:24:36 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.0 KB, free 366.0 MB)
17/04/02 17:24:36 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.7 KB, free 366.0 MB)
17/04/02 17:24:36 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:40882 (size: 3.7 KB, free: 366.3 MB)
17/04/02 17:24:36 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1012
17/04/02 17:24:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[21] at sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:24:36 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/04/02 17:24:36 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0, ANY, 5343 bytes)
17/04/02 17:24:36 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
17/04/02 17:24:36 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/04/02 17:24:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
17/04/02 17:24:36 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1873 bytes result sent to driver
17/04/02 17:24:36 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 32 ms on localhost (1/1)
17/04/02 17:24:36 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/04/02 17:24:36 INFO DAGScheduler: ResultStage 3 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.033 s
17/04/02 17:24:36 INFO DAGScheduler: Job 2 finished: sql at NativeMethodAccessorImpl.java:-2, took 0.361809 s
17/04/02 17:24:36 INFO CodeGenerator: Code generated in 7.637114 ms
17/04/02 17:24:36 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `iris`
17/04/02 17:24:36 INFO SparkSqlParser: Parsing command: count(1)
17/04/02 17:24:36 INFO CodeGenerator: Code generated in 15.873513 ms
17/04/02 17:24:36 INFO CodeGenerator: Code generated in 8.440347 ms
17/04/02 17:24:36 INFO SparkContext: Starting job: collect at utils.scala:195
17/04/02 17:24:36 INFO DAGScheduler: Registering RDD 25 (collect at utils.scala:195)
17/04/02 17:24:36 INFO DAGScheduler: Got job 3 (collect at utils.scala:195) with 1 output partitions
17/04/02 17:24:36 INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:195)
17/04/02 17:24:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
17/04/02 17:24:36 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)
17/04/02 17:24:36 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[25] at collect at utils.scala:195), which has no missing parents
17/04/02 17:24:36 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 15.6 KB, free 365.9 MB)
17/04/02 17:24:36 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.4 KB, free 365.9 MB)
17/04/02 17:24:36 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:40882 (size: 7.4 KB, free: 366.2 MB)
17/04/02 17:24:36 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1012
17/04/02 17:24:36 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[25] at collect at utils.scala:195)
17/04/02 17:24:36 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/04/02 17:24:36 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5981 bytes)
17/04/02 17:24:36 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
17/04/02 17:24:36 INFO BlockManager: Found block rdd_15_0 locally
17/04/02 17:24:36 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2141 bytes result sent to driver
17/04/02 17:24:36 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 13 ms on localhost (1/1)
17/04/02 17:24:36 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/04/02 17:24:36 INFO DAGScheduler: ShuffleMapStage 4 (collect at utils.scala:195) finished in 0.014 s
17/04/02 17:24:36 INFO DAGScheduler: looking for newly runnable stages
17/04/02 17:24:36 INFO DAGScheduler: running: Set()
17/04/02 17:24:36 INFO DAGScheduler: waiting: Set(ResultStage 5)
17/04/02 17:24:36 INFO DAGScheduler: failed: Set()
17/04/02 17:24:36 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[28] at collect at utils.scala:195), which has no missing parents
17/04/02 17:24:36 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 9.5 KB, free 365.9 MB)
17/04/02 17:24:36 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.1 KB, free 365.9 MB)
17/04/02 17:24:36 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:40882 (size: 4.1 KB, free: 366.2 MB)
17/04/02 17:24:36 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1012
17/04/02 17:24:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[28] at collect at utils.scala:195)
17/04/02 17:24:36 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
17/04/02 17:24:36 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0, ANY, 5335 bytes)
17/04/02 17:24:36 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
17/04/02 17:24:36 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/04/02 17:24:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/04/02 17:24:36 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2276 bytes result sent to driver
17/04/02 17:24:36 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 18 ms on localhost (1/1)
17/04/02 17:24:36 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/04/02 17:24:36 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:195) finished in 0.019 s
17/04/02 17:24:36 INFO DAGScheduler: Job 3 finished: collect at utils.scala:195, took 0.060062 s
17/04/02 17:24:36 INFO SparkSqlParser: Parsing command: SELECT *
FROM `iris` AS `zzz1`
WHERE (0 = 1)
17/04/02 17:24:36 INFO SparkSqlParser: Parsing command: Sepal_Length
17/04/02 17:24:36 INFO SparkSqlParser: Parsing command: Sepal_Width
17/04/02 17:24:36 INFO SparkSqlParser: Parsing command: Petal_Length
17/04/02 17:24:36 INFO SparkSqlParser: Parsing command: Petal_Width
17/04/02 17:24:36 INFO SparkSqlParser: Parsing command: Species
17/04/02 17:24:36 INFO SparkSqlParser: Parsing command: SELECT *
FROM `iris`
17/04/02 17:24:37 INFO NettyUtil: Found Netty's native epoll transport in the classpath, using it
17/04/02 17:24:37 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:40882 in memory (size: 7.7 KB, free: 366.3 MB)
17/04/02 17:24:37 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 127.0.0.1:40882 in memory (size: 3.7 KB, free: 366.3 MB)
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 194
17/04/02 17:24:37 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 127.0.0.1:40882 in memory (size: 7.4 KB, free: 366.3 MB)
17/04/02 17:24:37 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 127.0.0.1:40882 in memory (size: 4.1 KB, free: 366.3 MB)
17/04/02 17:24:37 INFO ContextCleaner: Cleaned shuffle 0
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 105
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 104
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 103
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 102
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 101
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 100
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 99
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 98
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 97
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 96
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 95
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 94
17/04/02 17:24:37 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:40882 in memory (size: 4.4 KB, free: 366.3 MB)
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 1
17/04/02 17:24:37 INFO ContextCleaner: Cleaned accumulator 0
17/04/02 17:24:37 INFO Cluster: New Cassandra host localhost/127.0.0.1:9042 added
17/04/02 17:24:37 INFO CassandraConnector: Connected to Cassandra cluster: Test Cluster
17/04/02 17:24:37 INFO SparkContext: Invoking stop() from shutdown hook
17/04/02 17:24:37 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4041
17/04/02 17:24:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/04/02 17:24:38 INFO MemoryStore: MemoryStore cleared
17/04/02 17:24:38 INFO BlockManager: BlockManager stopped
17/04/02 17:24:38 INFO BlockManagerMaster: BlockManagerMaster stopped
17/04/02 17:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/04/02 17:24:38 INFO SparkContext: Successfully stopped SparkContext
17/04/02 17:24:38 INFO ShutdownHookManager: Shutdown hook called
17/04/02 17:24:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-a63251a5-24a8-4aee-b4b8-d442369f34f9
17/04/02 17:24:40 INFO CassandraConnector: Disconnected from Cassandra cluster: Test Cluster
17/04/02 17:24:40 INFO SerialShutdownHooks: Successfully executed shutdown hook: Clearing session cache for C* connector
17/04/02 17:25:07 INFO SparkContext: Running Spark version 2.0.0
17/04/02 17:25:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/04/02 17:25:07 INFO SecurityManager: Changing view acls to: akhil
17/04/02 17:25:07 INFO SecurityManager: Changing modify acls to: akhil
17/04/02 17:25:07 INFO SecurityManager: Changing view acls groups to: 
17/04/02 17:25:07 INFO SecurityManager: Changing modify acls groups to: 
17/04/02 17:25:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(akhil); groups with view permissions: Set(); users  with modify permissions: Set(akhil); groups with modify permissions: Set()
17/04/02 17:25:07 INFO Utils: Successfully started service 'sparkDriver' on port 51007.
17/04/02 17:25:07 INFO SparkEnv: Registering MapOutputTracker
17/04/02 17:25:07 INFO SparkEnv: Registering BlockManagerMaster
17/04/02 17:25:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-be48101a-b6c0-4894-b050-ccb0ac7e4697
17/04/02 17:25:07 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/04/02 17:25:07 INFO SparkEnv: Registering OutputCommitCoordinator
17/04/02 17:25:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/04/02 17:25:08 INFO Utils: Successfully started service 'SparkUI' on port 4041.
17/04/02 17:25:08 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4041
17/04/02 17:25:08 INFO SparkContext: Added JAR file:/home/akhil/R/x86_64-pc-linux-gnu-library/3.3/sparklyr/java/sparklyr-2.0-2.11.jar at spark://127.0.0.1:51007/jars/sparklyr-2.0-2.11.jar with timestamp 1491150308236
17/04/02 17:25:08 INFO Executor: Starting executor ID driver on host localhost
17/04/02 17:25:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 47974.
17/04/02 17:25:08 INFO NettyBlockTransferService: Server created on 127.0.0.1:47974
17/04/02 17:25:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 47974)
17/04/02 17:25:08 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:47974 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 47974)
17/04/02 17:25:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 47974)
17/04/02 17:25:08 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/04/02 17:25:08 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/04/02 17:25:08 INFO HiveSharedState: Warehouse path is 'file:/home/akhil/example/crassy/tests/testthat/spark-warehouse'.
17/04/02 17:25:09 INFO NettyUtil: Found Netty's native epoll transport in the classpath, using it
17/04/02 17:25:10 INFO Cluster: New Cassandra host localhost/127.0.0.1:9042 added
17/04/02 17:25:10 INFO CassandraConnector: Connected to Cassandra cluster: Test Cluster
17/04/02 17:25:11 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/04/02 17:25:12 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/04/02 17:25:12 INFO ObjectStore: ObjectStore, initialize called
17/04/02 17:25:12 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/04/02 17:25:12 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/04/02 17:25:14 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/04/02 17:25:14 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:25:14 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:25:15 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:25:15 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:25:15 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/04/02 17:25:15 INFO ObjectStore: Initialized ObjectStore
17/04/02 17:25:15 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/04/02 17:25:15 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/04/02 17:25:16 INFO HiveMetaStore: Added admin role in metastore
17/04/02 17:25:16 INFO HiveMetaStore: Added public role in metastore
17/04/02 17:25:16 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/04/02 17:25:16 INFO HiveMetaStore: 0: get_all_databases
17/04/02 17:25:16 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_all_databases	
17/04/02 17:25:16 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/04/02 17:25:16 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/04/02 17:25:16 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/04/02 17:25:16 INFO SessionState: Created local directory: /tmp/91150887-0dac-4a92-9e93-8097a84b9914_resources
17/04/02 17:25:16 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/91150887-0dac-4a92-9e93-8097a84b9914
17/04/02 17:25:16 INFO SessionState: Created local directory: /tmp/akhil/91150887-0dac-4a92-9e93-8097a84b9914
17/04/02 17:25:16 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/91150887-0dac-4a92-9e93-8097a84b9914/_tmp_space.db
17/04/02 17:25:16 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/home/akhil/example/crassy/tests/testthat/spark-warehouse
17/04/02 17:25:16 INFO SessionState: Created local directory: /tmp/bd3d0eac-4035-426e-8d0c-685dcf5217f8_resources
17/04/02 17:25:16 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/bd3d0eac-4035-426e-8d0c-685dcf5217f8
17/04/02 17:25:16 INFO SessionState: Created local directory: /tmp/akhil/bd3d0eac-4035-426e-8d0c-685dcf5217f8
17/04/02 17:25:16 INFO SessionState: Created HDFS directory: /tmp/hive/akhil/bd3d0eac-4035-426e-8d0c-685dcf5217f8/_tmp_space.db
17/04/02 17:25:16 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/home/akhil/example/crassy/tests/testthat/spark-warehouse
17/04/02 17:25:16 INFO HiveMetaStore: 0: create_database: Database(name:default, description:default database, locationUri:file:/home/akhil/example/crassy/tests/testthat/spark-warehouse, parameters:{})
17/04/02 17:25:16 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/home/akhil/example/crassy/tests/testthat/spark-warehouse, parameters:{})	
17/04/02 17:25:17 INFO SparkSqlParser: Parsing command: spk_tbl
17/04/02 17:25:17 INFO SparkSqlParser: Parsing command: SELECT *
FROM `spk_tbl` AS `zzz1`
WHERE (0 = 1)
17/04/02 17:25:17 INFO CassandraConnector: Disconnected from Cassandra cluster: Test Cluster
17/04/02 17:25:17 INFO SparkSqlParser: Parsing command: role
17/04/02 17:25:17 INFO SparkSqlParser: Parsing command: can_login
17/04/02 17:25:18 INFO CodeGenerator: Code generated in 324.934194 ms
17/04/02 17:25:18 INFO SparkSqlParser: Parsing command: SELECT *
FROM `spk_tbl`
17/04/02 17:25:18 INFO SparkSqlParser: Parsing command: role
17/04/02 17:25:18 INFO SparkSqlParser: Parsing command: can_login
17/04/02 17:25:18 INFO CassandraSourceRelation: Input Predicates: []
17/04/02 17:25:18 INFO Cluster: New Cassandra host localhost/127.0.0.1:9042 added
17/04/02 17:25:18 INFO CassandraConnector: Connected to Cassandra cluster: Test Cluster
17/04/02 17:25:18 INFO CassandraSourceRelation: Input Predicates: []
17/04/02 17:25:18 INFO CodeGenerator: Code generated in 16.690838 ms
17/04/02 17:25:18 INFO SparkContext: Starting job: collect at utils.scala:195
17/04/02 17:25:18 INFO DAGScheduler: Got job 0 (collect at utils.scala:195) with 10 output partitions
17/04/02 17:25:18 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:195)
17/04/02 17:25:18 INFO DAGScheduler: Parents of final stage: List()
17/04/02 17:25:18 INFO DAGScheduler: Missing parents: List()
17/04/02 17:25:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at collect at utils.scala:195), which has no missing parents
17/04/02 17:25:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 11.5 KB, free 366.3 MB)
17/04/02 17:25:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.0 KB, free 366.3 MB)
17/04/02 17:25:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:47974 (size: 6.0 KB, free: 366.3 MB)
17/04/02 17:25:18 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
17/04/02 17:25:19 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at collect at utils.scala:195)
17/04/02 17:25:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks
17/04/02 17:25:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, NODE_LOCAL, 9826 bytes)
17/04/02 17:25:19 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, NODE_LOCAL, 9828 bytes)
17/04/02 17:25:19 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, NODE_LOCAL, 10424 bytes)
17/04/02 17:25:19 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, NODE_LOCAL, 10184 bytes)
17/04/02 17:25:19 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/04/02 17:25:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/04/02 17:25:19 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
17/04/02 17:25:19 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
17/04/02 17:25:19 INFO Executor: Fetching spark://127.0.0.1:51007/jars/sparklyr-2.0-2.11.jar with timestamp 1491150308236
17/04/02 17:25:19 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:51007 after 10 ms (0 ms spent in bootstraps)
17/04/02 17:25:19 INFO Utils: Fetching spark://127.0.0.1:51007/jars/sparklyr-2.0-2.11.jar to /tmp/spark-0a0a970f-405e-4f4e-ac1c-991ceb548746/userFiles-575ee222-45fb-4055-b02b-d28e78e2f7df/fetchFileTemp596097732648638041.tmp
17/04/02 17:25:19 INFO Executor: Adding file:/tmp/spark-0a0a970f-405e-4f4e-ac1c-991ceb548746/userFiles-575ee222-45fb-4055-b02b-d28e78e2f7df/sparklyr-2.0-2.11.jar to class loader
17/04/02 17:25:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1115 bytes result sent to driver
17/04/02 17:25:19 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1115 bytes result sent to driver
17/04/02 17:25:19 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1202 bytes result sent to driver
17/04/02 17:25:19 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, partition 4, NODE_LOCAL, 10541 bytes)
17/04/02 17:25:19 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
17/04/02 17:25:19 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, partition 5, NODE_LOCAL, 10541 bytes)
17/04/02 17:25:19 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
17/04/02 17:25:19 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1115 bytes result sent to driver
17/04/02 17:25:19 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, partition 6, NODE_LOCAL, 10657 bytes)
17/04/02 17:25:19 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
17/04/02 17:25:19 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, partition 7, NODE_LOCAL, 9944 bytes)
17/04/02 17:25:19 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
17/04/02 17:25:19 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 554 ms on localhost (1/10)
17/04/02 17:25:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 604 ms on localhost (2/10)
17/04/02 17:25:19 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 575 ms on localhost (3/10)
17/04/02 17:25:19 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 571 ms on localhost (4/10)
17/04/02 17:25:19 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1188 bytes result sent to driver
17/04/02 17:25:19 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, localhost, partition 8, NODE_LOCAL, 9350 bytes)
17/04/02 17:25:19 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1188 bytes result sent to driver
17/04/02 17:25:19 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 252 ms on localhost (5/10)
17/04/02 17:25:19 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, localhost, partition 9, NODE_LOCAL, 7673 bytes)
17/04/02 17:25:19 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
17/04/02 17:25:19 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 241 ms on localhost (6/10)
17/04/02 17:25:19 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
17/04/02 17:25:19 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1188 bytes result sent to driver
17/04/02 17:25:19 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 277 ms on localhost (7/10)
17/04/02 17:25:19 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1115 bytes result sent to driver
17/04/02 17:25:19 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 102 ms on localhost (8/10)
17/04/02 17:25:19 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1386 bytes result sent to driver
17/04/02 17:25:19 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 344 ms on localhost (9/10)
17/04/02 17:25:19 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1115 bytes result sent to driver
17/04/02 17:25:19 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 136 ms on localhost (10/10)
17/04/02 17:25:19 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:195) finished in 0.945 s
17/04/02 17:25:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/04/02 17:25:19 INFO DAGScheduler: Job 0 finished: collect at utils.scala:195, took 1.089942 s
17/04/02 17:25:20 INFO CodeGenerator: Code generated in 18.420012 ms
17/04/02 17:25:20 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/04/02 17:25:20 INFO HiveMetaStore: 0: get_database: default
17/04/02 17:25:20 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_database: default	
17/04/02 17:25:20 INFO HiveMetaStore: 0: get_database: default
17/04/02 17:25:20 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_database: default	
17/04/02 17:25:20 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/04/02 17:25:20 INFO audit: ugi=akhil	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/04/02 17:25:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:47974 in memory (size: 6.0 KB, free: 366.3 MB)
17/04/02 17:25:20 INFO CodeGenerator: Code generated in 14.866071 ms
17/04/02 17:25:20 INFO SparkContext: Starting job: collect at utils.scala:59
17/04/02 17:25:20 INFO DAGScheduler: Got job 1 (collect at utils.scala:59) with 1 output partitions
17/04/02 17:25:20 INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:59)
17/04/02 17:25:20 INFO DAGScheduler: Parents of final stage: List()
17/04/02 17:25:20 INFO DAGScheduler: Missing parents: List()
17/04/02 17:25:20 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at map at utils.scala:56), which has no missing parents
17/04/02 17:25:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.3 KB, free 366.3 MB)
17/04/02 17:25:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KB, free 366.3 MB)
17/04/02 17:25:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:47974 (size: 4.4 KB, free: 366.3 MB)
17/04/02 17:25:20 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1012
17/04/02 17:25:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at map at utils.scala:56)
17/04/02 17:25:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/04/02 17:25:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 10, localhost, partition 0, PROCESS_LOCAL, 5686 bytes)
17/04/02 17:25:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 10)
17/04/02 17:25:20 INFO CodeGenerator: Code generated in 9.096676 ms
17/04/02 17:25:20 INFO Executor: Finished task 0.0 in stage 1.0 (TID 10). 1072 bytes result sent to driver
17/04/02 17:25:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 10) in 25 ms on localhost (1/1)
17/04/02 17:25:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/04/02 17:25:20 INFO DAGScheduler: ResultStage 1 (collect at utils.scala:59) finished in 0.026 s
17/04/02 17:25:20 INFO DAGScheduler: Job 1 finished: collect at utils.scala:59, took 0.035027 s
17/04/02 17:25:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 70.7 KB, free 366.2 MB)
17/04/02 17:25:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 22.9 KB, free 366.2 MB)
17/04/02 17:25:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:47974 (size: 22.9 KB, free: 366.3 MB)
17/04/02 17:25:20 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:-2
17/04/02 17:25:20 INFO FileInputFormat: Total input paths to process : 1
17/04/02 17:25:20 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:-2
17/04/02 17:25:20 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/04/02 17:25:20 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:-2)
17/04/02 17:25:20 INFO DAGScheduler: Parents of final stage: List()
17/04/02 17:25:20 INFO DAGScheduler: Missing parents: List()
17/04/02 17:25:20 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at csv at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/04/02 17:25:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 366.2 MB)
17/04/02 17:25:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.0 KB, free 366.2 MB)
17/04/02 17:25:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:47974 (size: 2.0 KB, free: 366.3 MB)
17/04/02 17:25:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1012
17/04/02 17:25:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at csv at NativeMethodAccessorImpl.java:-2)
17/04/02 17:25:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/04/02 17:25:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 11, localhost, partition 0, PROCESS_LOCAL, 5502 bytes)
17/04/02 17:25:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 11)
17/04/02 17:25:20 INFO HadoopRDD: Input split: file:/tmp/Rtmp6hHzZl/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2013
17/04/02 17:25:20 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/04/02 17:25:20 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/04/02 17:25:20 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/04/02 17:25:20 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/04/02 17:25:20 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/04/02 17:25:20 INFO Executor: Finished task 0.0 in stage 2.0 (TID 11). 986 bytes result sent to driver
17/04/02 17:25:20 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 11) in 54 ms on localhost (1/1)
17/04/02 17:25:20 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:-2) finished in 0.056 s
17/04/02 17:25:20 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/04/02 17:25:20 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:-2, took 0.065819 s
17/04/02 17:25:20 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 232.8 KB, free 366.0 MB)
17/04/02 17:25:20 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 22.9 KB, free 365.9 MB)
17/04/02 17:25:20 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:47974 (size: 22.9 KB, free: 366.2 MB)
17/04/02 17:25:20 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:-2
17/04/02 17:25:21 INFO SparkSqlParser: Parsing command: iris
17/04/02 17:25:21 INFO SparkSqlParser: Parsing command: CACHE TABLE `iris`
17/04/02 17:25:21 INFO SparkSqlParser: Parsing command: `iris`
17/04/02 17:25:21 INFO FileSourceStrategy: Pruning directories with: 
17/04/02 17:25:21 INFO FileSourceStrategy: Post-Scan Filters: 
17/04/02 17:25:21 INFO FileSourceStrategy: Pruned Data Schema: struct<Sepal_Length: double, Sepal_Width: double, Petal_Length: double, Petal_Width: double, Species: string ... 3 more fields>
17/04/02 17:25:21 INFO FileSourceStrategy: Pushed Filters: 
17/04/02 17:25:21 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 269.0 KB, free 365.7 MB)
17/04/02 17:25:21 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.4 KB, free 365.7 MB)
17/04/02 17:25:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:47974 (size: 23.4 KB, free: 366.2 MB)
17/04/02 17:25:21 INFO SparkContext: Created broadcast 5 from sql at NativeMethodAccessorImpl.java:-2
17/04/02 17:25:21 INFO FileSourceStrategy: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/04/02 17:25:21 INFO CodeGenerator: Code generated in 19.791372 ms
17/04/02 17:25:21 INFO CodeGenerator: Code generated in 9.920791 ms
17/04/02 17:25:21 INFO CodeGenerator: Code generated in 8.814178 ms
17/04/02 17:25:21 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:-2
17/04/02 17:25:21 INFO DAGScheduler: Registering RDD 24 (sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:25:21 INFO DAGScheduler: Got job 3 (sql at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/04/02 17:25:21 INFO DAGScheduler: Final stage: ResultStage 4 (sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:25:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
17/04/02 17:25:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
17/04/02 17:25:21 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[24] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/04/02 17:25:21 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 16.4 KB, free 365.6 MB)
17/04/02 17:25:21 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.7 KB, free 365.6 MB)
17/04/02 17:25:21 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:47974 (size: 7.7 KB, free: 366.2 MB)
17/04/02 17:25:21 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1012
17/04/02 17:25:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[24] at sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:25:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/04/02 17:25:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 12, localhost, partition 0, PROCESS_LOCAL, 5989 bytes)
17/04/02 17:25:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 12)
17/04/02 17:25:21 INFO FileScanRDD: Reading File path: file:///tmp/Rtmp6hHzZl/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv, range: 0-4026, partition values: [empty row]
17/04/02 17:25:21 INFO CodeGenerator: Code generated in 16.828426 ms
17/04/02 17:25:21 INFO MemoryStore: Block rdd_21_0 stored as values in memory (estimated size 5.6 KB, free 365.6 MB)
17/04/02 17:25:21 INFO BlockManagerInfo: Added rdd_21_0 in memory on 127.0.0.1:47974 (size: 5.6 KB, free: 366.2 MB)
17/04/02 17:25:21 INFO CodeGenerator: Code generated in 19.425117 ms
17/04/02 17:25:21 INFO CodeGenerator: Code generated in 44.223033 ms
17/04/02 17:25:21 INFO Executor: Finished task 0.0 in stage 3.0 (TID 12). 3567 bytes result sent to driver
17/04/02 17:25:21 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 12) in 292 ms on localhost (1/1)
17/04/02 17:25:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/04/02 17:25:21 INFO DAGScheduler: ShuffleMapStage 3 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.293 s
17/04/02 17:25:21 INFO DAGScheduler: looking for newly runnable stages
17/04/02 17:25:21 INFO DAGScheduler: running: Set()
17/04/02 17:25:21 INFO DAGScheduler: waiting: Set(ResultStage 4)
17/04/02 17:25:21 INFO DAGScheduler: failed: Set()
17/04/02 17:25:21 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[27] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/04/02 17:25:21 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 7.0 KB, free 365.6 MB)
17/04/02 17:25:21 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.7 KB, free 365.6 MB)
17/04/02 17:25:21 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:47974 (size: 3.7 KB, free: 366.2 MB)
17/04/02 17:25:21 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1012
17/04/02 17:25:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[27] at sql at NativeMethodAccessorImpl.java:-2)
17/04/02 17:25:21 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/04/02 17:25:21 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 13, localhost, partition 0, ANY, 5343 bytes)
17/04/02 17:25:21 INFO Executor: Running task 0.0 in stage 4.0 (TID 13)
17/04/02 17:25:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/04/02 17:25:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
17/04/02 17:25:21 INFO Executor: Finished task 0.0 in stage 4.0 (TID 13). 1873 bytes result sent to driver
17/04/02 17:25:21 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 13) in 41 ms on localhost (1/1)
17/04/02 17:25:21 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/04/02 17:25:21 INFO DAGScheduler: ResultStage 4 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.041 s
17/04/02 17:25:21 INFO DAGScheduler: Job 3 finished: sql at NativeMethodAccessorImpl.java:-2, took 0.412898 s
17/04/02 17:25:21 INFO CodeGenerator: Code generated in 6.65464 ms
17/04/02 17:25:21 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `iris`
17/04/02 17:25:21 INFO SparkSqlParser: Parsing command: count(1)
17/04/02 17:25:21 INFO CodeGenerator: Code generated in 13.343671 ms
17/04/02 17:25:21 INFO CodeGenerator: Code generated in 15.682644 ms
17/04/02 17:25:21 INFO SparkContext: Starting job: collect at utils.scala:195
17/04/02 17:25:21 INFO DAGScheduler: Registering RDD 31 (collect at utils.scala:195)
17/04/02 17:25:21 INFO DAGScheduler: Got job 4 (collect at utils.scala:195) with 1 output partitions
17/04/02 17:25:21 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:195)
17/04/02 17:25:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/04/02 17:25:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/04/02 17:25:21 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[31] at collect at utils.scala:195), which has no missing parents
17/04/02 17:25:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 15.6 KB, free 365.6 MB)
17/04/02 17:25:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 7.4 KB, free 365.6 MB)
17/04/02 17:25:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:47974 (size: 7.4 KB, free: 366.2 MB)
17/04/02 17:25:21 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1012
17/04/02 17:25:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[31] at collect at utils.scala:195)
17/04/02 17:25:21 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
17/04/02 17:25:21 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 14, localhost, partition 0, PROCESS_LOCAL, 5981 bytes)
17/04/02 17:25:21 INFO Executor: Running task 0.0 in stage 5.0 (TID 14)
17/04/02 17:25:21 INFO BlockManager: Found block rdd_21_0 locally
17/04/02 17:25:21 INFO Executor: Finished task 0.0 in stage 5.0 (TID 14). 2141 bytes result sent to driver
17/04/02 17:25:21 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 14) in 13 ms on localhost (1/1)
17/04/02 17:25:21 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/04/02 17:25:21 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:195) finished in 0.014 s
17/04/02 17:25:21 INFO DAGScheduler: looking for newly runnable stages
17/04/02 17:25:21 INFO DAGScheduler: running: Set()
17/04/02 17:25:21 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/04/02 17:25:21 INFO DAGScheduler: failed: Set()
17/04/02 17:25:21 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[34] at collect at utils.scala:195), which has no missing parents
17/04/02 17:25:21 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.5 KB, free 365.6 MB)
17/04/02 17:25:22 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 365.6 MB)
17/04/02 17:25:22 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 127.0.0.1:47974 (size: 4.1 KB, free: 366.2 MB)
17/04/02 17:25:22 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1012
17/04/02 17:25:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[34] at collect at utils.scala:195)
17/04/02 17:25:22 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/04/02 17:25:22 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 15, localhost, partition 0, ANY, 5335 bytes)
17/04/02 17:25:22 INFO Executor: Running task 0.0 in stage 6.0 (TID 15)
17/04/02 17:25:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/04/02 17:25:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/04/02 17:25:22 INFO Executor: Finished task 0.0 in stage 6.0 (TID 15). 2189 bytes result sent to driver
17/04/02 17:25:22 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 15) in 16 ms on localhost (1/1)
17/04/02 17:25:22 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/04/02 17:25:22 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:195) finished in 0.018 s
17/04/02 17:25:22 INFO DAGScheduler: Job 4 finished: collect at utils.scala:195, took 0.051419 s
17/04/02 17:25:22 INFO SparkSqlParser: Parsing command: SELECT *
FROM `iris` AS `zzz2`
WHERE (0 = 1)
17/04/02 17:25:22 INFO SparkSqlParser: Parsing command: Sepal_Length
17/04/02 17:25:22 INFO SparkSqlParser: Parsing command: Sepal_Width
17/04/02 17:25:22 INFO SparkSqlParser: Parsing command: Petal_Length
17/04/02 17:25:22 INFO SparkSqlParser: Parsing command: Petal_Width
17/04/02 17:25:22 INFO SparkSqlParser: Parsing command: Species
17/04/02 17:25:22 INFO SparkSqlParser: Parsing command: SELECT *
FROM `iris`
17/04/02 17:25:22 INFO SparkContext: Invoking stop() from shutdown hook
17/04/02 17:25:22 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4041
17/04/02 17:25:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/04/02 17:25:22 INFO MemoryStore: MemoryStore cleared
17/04/02 17:25:22 INFO BlockManager: BlockManager stopped
17/04/02 17:25:22 INFO BlockManagerMaster: BlockManagerMaster stopped
17/04/02 17:25:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/04/02 17:25:22 INFO SparkContext: Successfully stopped SparkContext
17/04/02 17:25:22 INFO ShutdownHookManager: Shutdown hook called
17/04/02 17:25:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-0a0a970f-405e-4f4e-ac1c-991ceb548746
17/04/02 17:25:24 INFO CassandraConnector: Disconnected from Cassandra cluster: Test Cluster
17/04/02 17:25:24 INFO SerialShutdownHooks: Successfully executed shutdown hook: Clearing session cache for C* connector
